{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww12860\viewh11040\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs36 \cf0 There are three lectures, two labs, a homework, some datasets and pictures.\
\
A few notes follow\
\
** FILE FORMATS **\
I have used Rmd, but did not find it a particularly useful tool. On the one hand, the slides do not look as good as they could (sizing the figures is challenging), they are not interactive, and writing invisible R code is of limited interest. On the other hand, if the students are supposed to look at the .Rmd document for the labs, they will find it full of odd annotation and commands. \
So, I have used Rmd for slides, but for the first  and last lecture, I have often imported figures created in R and saved as PDF rather then having R plotting them as the Rmd file is compiled.\
In the second lecture, where I show how to do PCA in R I am using Rmd in more explicit form.\
The labs are in .R, so that the students only see R commands and comment lines.\
\
** LECTURE CONTENTS **\
I have tried to pitch PCA to a freshman audience, minimizing the use of mathematical elements that they might not know. The progression of the three lectures is as follows\
\
Lecture 1: the first principal component is introduced as the line through a cloud of data in two dimension that minimizes the sum of squared euclidean distances.\
We see that this line behaves differently from the regression line and it is in some setting better to capture the \'93underlying relation\'94 between variables. At the conclusion of this lecture, we see how this is equivalent to looking for a line a projection onto which has maximal variance.\
\
Lecture 2: we now introduce the first principal component as the linear combination of multiple variables with maximal variance. We do an eigenvector eigenvalue problem, and we see how to calculate this in R.\
There is no mention of SVD. I do not think this is needed\
\
Lecture 3: we consider two aspects. 1) When we are looking at data in high dimension, it might be useful to look at the projection not only on the first principal component, but also on some of the remaining ones.\
2) We discuss the dangers of reification and over interpretation of PCA.\
\
** LAB content **\
\
Lab 1 is substantially going over the code behind lecture 1: they will review regression lines, how to calculate distances of a point to a line etc. All the commands used to create figures in lecture 1 are here.\
\
Lab 2 is running PCA analysis on two datasets: the decathlon which is described in Lecture 2 and a senate votes dataset.\
Finally, the students generate a dataset that has lower rank and see what happens when you do PCA on it.\
\
\
** Homework **\
\
The students are invited to run PCA analysis on  one of two (possibly three) datasets.\
a) a larger version of the personality questionnaire, where there are 100 items rather then 20\
b) a different voting dataset that they might put together looking at the congress web-site\
c) a genetic dataset with genotypes of individuals from many different populations. The format in which I got the data requires a fair amount of massaging and I have not been able to do it yet. I expect to finish soon, and then I will include it in the homework, as well as possibly in the lectures.\
 \
}