---
title: 'Prediction problems 1: Overview and Linear Regression'
author: "Data Science 101 Team"
date: "May 15, 2016"
output:
  slidy_presentation:
    css: styles.css
    keep_md: yes
  ioslides_presentation: default
---

```{r setup, include=FALSE}
#source('http://datascience101.stanford.edu/profile.R')
knitr::opts_chunk$set(echo = FALSE,
                      root.dir="./")
```

## Learning Goals

- What is a prediction problem?
- Defining a model
- Fitting a model
- Validating your model (does it work?)


## Outline

- Examples and motivation
- Linear regression with a single variable
    + Example: predicting heights of children by height of parents
	+ The model
	+ Fitting the model
- Linear regression with multiple variables
	+ Example: housing data from Boston
	+ The general model
	+ Fitting the model

## Examples 


- DNA string binding prediction ($x$ = string of ACTG, $y$ = binding
affinities)
- Disease prediction (person has symptoms $x$, what disease does he/she have?
- Prediction of stock returns from past returns, and info about the company, economy etc
- Car insurance: $y$=chance of an accident, $x=$ characteritsitcs of driver (age, gender, driving  record)
- Elections: $y$ vote for candidate A or B;  $x$=previous voting results,  results of polls,  economic conditions


Also known as "supervised learning"

## Linear regression with a single variable

```{r echo=TRUE}
heights = read.csv("data/galton.csv", row.names = 1);
plot(heights$parent, heights$child,
     pch = 19, cex = .25, xlab="", ylab="", asp=1);
title(xlab = "Parent height", ylab = "Child height");
```

* Goal: predict child height $y$ from parent's height $x$
	+ Call $y$ the <font class = "emphred">dependent variable</font>
    + Call $x$ the <font class = "emphred">independent variable</font>

## Parent height from child heights

- First idea: predict
$$
\widehat{y} = x
$$
```{r echo=TRUE}
plot(heights$parent, heights$child,
     pch = 19, cex = .25, xlab="", ylab="", asp=1);
lines(c(63.2, 73), c(63.2, 73), col="red",  # Plot straight line
     type="l", lwd=3, asp=1, main = "", xlab = "", ylab="");
title(xlab = "Parent height", ylab = "Child height");
```

## Parent height from child heights

- Probably a better idea: predict
$$
	\widehat{y} = \underbrace{\beta_0}_{\rm intercept}
		+ \underbrace{\beta_1}_{\rm slope} x
$$
- The <font class="emphblue">best fit</font> line of this form

```{r heights_with_lines}
plot(heights$parent, heights$child,
     pch = 19, cex = .25, xlab="", ylab="", asp=1);
lines(c(63.2, 73), c(63.2, 73), col="red",
     type="l", lwd=3, asp=1, main = "", xlab = "", ylab="");
linreg = lm(formula = heights$child ~ heights$parent);
lines(heights$parent, linreg$fitted.values, col="blue", lwd=4);
title(xlab = "Parent height", ylab = "Child height");
```

## Linear regression model

- Defining the model (according to George Box)

> All models are wrong; some models are useful...
> Just as the ability to devise simple but evocative models is the signature
> of the great scientist so overelaboration and overparameterization is
> often the mark of mediocrity
		
- Often assume a *linear* model, that is, that the *response* or *target* $y$
satisfies
$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

### Terms above
- $\beta_0$ is the *intercept* (or *bias*) term
- $\beta_1$ is the *slope* or *coefficients of the model*
- $\varepsilon$ is a noise term, which is usually assumed independent of $x$ and mean zero


## Linear regression (parent heights)

\[
\begin{aligned}
\hat{y} &  = \beta_0 ~~~~~ + ~~~~~~
\beta_1 x \\
& = \beta_0 + \beta_1 \overline{x} + \beta_1 \cdot (x - \overline{x})
\end{aligned}
\]
where $\overline{x}$ is the mean of all parent heights


### Meaning of these?

Suppose the slope $\beta_1 < 1$

- $\beta_0 + \beta_1 \overline{x}$ is average child height
- $\beta_1 \cdot (x - \overline{x})$ is *regression to mean*, so taller
parents' children shrink toward average

## Fitting the regression model

- **Data**: Have $n$ pairs $(x_i, y_i)$, where $x_i$ is the height of
parent $i$ and $y_i$ is height of child $i$
- **Predictions**: $\hat{y}_i$ is our model's prediction of child height $i$
- **Loss function**: First, define a way to measure *error* or *residual* $\hat{y}_i - y_i$

$$
	Loss(y, \hat{y}) = (y - \hat{y})^2
$$

```{r}
library(graphics)
x = seq(-2, 2, len=101);
y = x^2 / 2;
matplot(x, y, type="l", xlab = expression(y - hat(y)),
        ylab = expression(paste("Loss(", y, ", ", hat(y), ")")),
        cex.lab = 1.5)
```

## Fitting the regression model

- **Data**: Have a lot of pairs $(x_i, y_i)$, where $x_i$ is the height of
parent $i$ and $y_i$ is height of child $i$
- **Predictions**: $\hat{y}_i = \beta_0 + \beta_1 x_i$ is model prediction of child height $i$
- **Loss on data**: Choose parameters $\beta_0$ and $\beta_1$ to solve
<font class = emphred>least squares</font> problem
$$
\mathop{\rm minimize}_{\beta_0, \beta_1}
\sum_{i = 1}^n (y_i - \hat{y}_i)^2
= \sum_{i = 1}^n (y_i - \beta_0 - \beta_1 x_i)^2
$$

```
linreg = lm(formula = y ~ x)
```
- `lm` is the code for "linear model", while the `~` symbol means to regress
  `y` on `x`

## Example: height data
```{r echo=TRUE}
## Read in data, with names in first row
heights = read.csv("data/galton.csv", row.names = 1);
linreg = lm(formula = heights$child ~ heights$parent);
```

Now that we have performed the regression, we can plot the result. The values that
our linear regression predicts are in `linreg$fitted.values`
```
plot(heights$parent, heights$child,
     pch = 19, cex = .25, xlab="", ylab="", asp=1);
lines(heights$parent, linreg$fitted.values, col = "blue", lwd=4);
title(xlab = "Parent height", ylab = "Child height");
```
```{r echo=FALSE}
plot(heights$parent, heights$child,
     pch = 19, cex = .25, xlab="", ylab="", asp=1);
lines(heights$parent, linreg$fitted.values, col = "blue", lwd=4);
title(xlab = "Parent height", ylab = "Child height");
legend(70, 65, legend = c("regression", "heights"), lty = c(1, NA),
       pch = c(NA, 19), col = c("blue", "black"),
       pt.cex = c(1, .25), pt.lwd = c(4, 1))
```

## Residuals of best fit

Compare squared residuals $(y - \hat{y})^2$ , to those  obtained by  predicting with parent's
height only
```{r echo=TRUE}
mean.squared.naive.residual = mean((heights$parent - heights$child)^2);
mean.squared.fit.residual = mean(linreg$residuals^2);
cat(paste("Mean squared residuals for naive prediction:", mean.squared.naive.residual,
          "\n"));
cat(paste("Mean squared residuals for fit prediction:", mean.squared.fit.residual,
          "\n"));
```
```
plot(heights$parent, heights$child,
     pch = 19, cex = .25, xlab="", ylab="", asp=1);
lines(heights$parent, linreg$fitted.values, col = "blue", lwd=4);
lines(heights$parent, heights$parent, col="red", lwd=4);
```
```{r echo=FALSE}
plot(heights$parent, heights$child,
     pch = 19, cex = .25, xlab="", ylab="", asp=1);
lines(heights$parent, linreg$fitted.values, col = "blue", lwd=4);
lines(heights$parent, heights$parent, col="red", lwd=4);
legend(69, 65,
       legend = c("optimal regression line", "naive prediction", "true height"),
       lty = c(1, 1, NA),
       pch = c(NA, NA, 19), col = c("blue", "red", "black"),
       pt.cex = c(1, 1, .25), pt.lwd = c(4, 4, 1))

```

## More general (multiple) regression

Instead of $x \in \mathbb{R}$, input <font class="emphred">vectors</font>
$x \in \mathbb{R}^p$

- $p$ is number of <font class = "emphred">parameters</font> (sometimes
use $d$ for *dimension*)
- Inputs of form
\[
	x = \left[\begin{matrix} x_1 \\ x_2 \\ \vdots \\ x_p \end{matrix} \right]
\]
- Call $x$ the *covariates*, *independent variables*, *explanatory variables*, *features*,
or *predictor variables*
    * Note that variables are usually NOT independent of one another

## Example: Housing data from Boston

- Target variable $y$ is median home price in a small area of Boston
- Input variables include
	* Crime rate per capita
	* Average number of bedrooms
	* Proportion of large lots (zoned for > 25,000 feet)
	* Whether a home is near the Charles river ($x \in \{0, 1\}$)

<div align="center">
<img src="figs/boston_bay.jpg" width=700>
</div>

<!-- TODO: Get the citations correct -->

## Example: Housing data from Boston

Let's do a little data exploration
```{r echo=TRUE}
library(MASS);
data(Boston);
boston = Boston;
## Remove the $500,000 sale price, as it is censored data we ignore
boston = boston[boston$medv != 50, ];
## Plot four of the individual predictors against price
par(mfrow = c(2, 2));
plot(boston$rm, boston$medv, xlab = "Number of bedrooms",
     ylab = "Sale price", pch=19, cex=.15);
plot(boston$chas, boston$medv, xlab = "Near Charles?",
     ylab = "Sale price", pch=19, cex=.15);
plot(boston$ptratio, boston$medv, xlab = "Student/teacher ratio",
     ylab = "Sale price", pch=19, cex=.15);
plot(boston$zn, boston$medv, xlab = "Large lot proportion",
     ylab = "Sale price", pch=19, cex=.15);
```

## Example (continued): Boston Housing

Idea: let us run a regression on each of these, and see which is best

- To run a regression on a dataframe, we tell R to use a formula with the
  columns we care about and the dataframe we want
- For example, to predict median value `boston$medv` from number of bedrooms
`boston$rm`, use
```
lm(formula = medv ~ rm, data = boston)
```

## Boston housing: rooms

```{r echo=TRUE}
linreg = lm(formula = medv ~ rm, data = boston);
plot(boston$rm, boston$medv, xlab = "Number of rooms",
     ylab = "Sale price", pch=19, cex=.25);
lines(boston$rm, linreg$fitted.values, col="red", lw = 2);
cat(paste("Mean squared error: ", mean(linreg$residuals^2)), "\n");
```

## Boston housing: Charles River

```{r echo=TRUE}
plot(boston$chas, boston$medv, xlab = "Near Charles?",
     ylab = "Sale price", pch=19, cex=.15);
linreg = lm(formula = medv ~ chas, data = boston);
lines(boston$chas, linreg$fitted.values, col = "red", lw = 2);
cat(paste("Mean squared error: ", mean(linreg$residuals^2)), "\n");
```

## Boston housing: Student to teacher ratio
```{r echo=TRUE}
plot(boston$ptratio, boston$medv, xlab = "Student/teacher ratio",
     ylab = "Sale price", pch=19, cex=.15);
linreg = lm(formula = medv ~ ptratio, data = boston);
lines(boston$ptratio, linreg$fitted.values, col = "red", lw = 2);
cat(paste("Mean squared error: ", mean(linreg$residuals^2)), "\n");
```

## Combining input variables

- Recall *dot product* for vectors $x$ and $v$
\[
	x \cdot v = x^T v = \sum_{j = 1}^p x_j v_j
\]
- Make predictions
\[
	\hat{y} = \beta_0 + \sum_{j = 1}^p x_j \beta_j
\]
- Model the data as
\[
	\begin{aligned}
	y & = \beta_0 + \sum_{j = 1}^p x_j \beta_j + \varepsilon \\
		& = \beta_0 + \beta \cdot x + \varepsilon
			= \beta_0 + \beta^T x + \varepsilon
	\end{aligned}
\]
	where $\varepsilon$ is noise
- Often, choose noise distribution based on
	+ Convenience (assume it is normally distributed)
	 - Prior knowledge (rarely)

## Fitting a multiple regression

- Making predictions using
\[
\hat{y} = \beta_0 + \sum_{j = 1}^p \beta_j x_j =
	\beta_0 + \beta \cdot x
\]
- Fit as in single variable case. Solve
$$
\mathop{\rm minimize}_{\beta_0 \in \mathbb{R}, \beta \in \mathbb{R}^p}
\sum_{i = 1}^n (y_i - \hat{y}_i)^2
= \sum_{i = 1}^n (y_i - \beta_0 - \beta \cdot x_i)^2
$$
	
## Housing data one at a time

```{r echo=TRUE}
par(mfrow = c(1, 2));
reg.rm = lm(formula = medv ~ rm, data = boston);
plot(boston$rm, boston$medv, xlab = "Number of rooms",
     ylab = "Sale price", pch=19, cex=.15,
     main = paste("Mean Sq. Error", round(mean(reg.rm$residuals^2), digits=2)));
lines(boston$rm, reg.rm$fitted.values, col="red", lw = 2);
reg.ptratio = lm(formula = medv ~ ptratio, data = boston);
plot(boston$ptratio, boston$medv, xlab = "Student/teacher ratio",
     ylab = "Sale price", pch=19, cex=.15,
     main = paste("Mean Sq. Error",
         round(mean(reg.ptratio$residuals^2), digits=2)));
lines(boston$ptratio, reg.ptratio$fitted.values, col="red", lw = 2);
```

## Housing data regression on two variables

To perform linear regression on multiple variables, in the `lm` command, simply
"add" them in the formula
```{r echo=TRUE}
reg.both = lm(formula = medv ~ rm + ptratio, data = boston)
cat(paste("Mean squared error (just rooms):",
          round(mean(reg.rm$residuals^2), digits = 2), "\n"));
cat(paste("Mean squared error (just student/teacher):",
          round(mean(reg.ptratio$residuals^2), digits = 2), "\n"));
cat(paste("Mean squared error (both):",
          round(mean(reg.both$residuals^2), digits = 2), "\n"));
```

## Housing data: regression on everything

To use all independent variables (except the target $y$ in the regression),
use the notation
```
lm(y ~ ., data = your.dataset)
```
For the Boston housing data,
```{r echo=TRUE}
fullregression = lm(formula = medv ~ ., data = boston)
plot(fullregression$fitted.values, boston$medv, type = "p", pch = 4,
     col = rgb(0, 0, 1),
     xlab = "Predicted price", ylab = "Actual price");
points(reg.rm$fitted.values, boston$medv, pch = 2,
       col = rgb(1, .5, .1));
legend(x = 0, y = 40, legend = c("All variables", "Only rooms"),
       col = c(rgb(0, 0, 1), rgb(1, .5, .1)), pch = c(4, 2));
cat(paste("Mean squared error for all:",
          round(mean(fullregression$residuals^2), digits=3), "\n"));
```
##Mean squared error on the (training) dataset does not tell the whole story

-it tells us how well the model performs on the SAME data that was use to fit it
-by overfitting the model (eg using lots of predictors), we can make the mean squared error
arbitrarily low

-but this model will not accurately predict future datasets.
Prediction of future datasets is called "generalization".

-a good strategy:  divide your dataset at random into say two parts--- a training (fitting) part and a validation part.

-fit your model using the training set, and record the mean squared error on the
validation part. Overfitting will exhibit itself as high mean squared error on the validation set.

## Summary

- Linear regression
	+ Predicted real-valued targets using *linear* predictions
	+ Learned to fit model using R
- Things we have *not* covered (but will explore)
	+ Choosing variables (variable selection)
	+ Encoding variables (is linear correct?)
	+ Interactions between variables
