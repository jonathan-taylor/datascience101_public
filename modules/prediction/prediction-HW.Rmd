---
title: "Homework for Prediction module"
author: "Data Science Team"
output:
  pdf_document: default
  html_document:
    keep_md: yes
---

```{r message=FALSE}
library(pacman)
```

# Due Friday May 26 at 9:30am via canvas.

## Part 1

I collect a set of data ($n=100$ observations) containing a single predictor 
and a quantitative response. I then fit a linear regression model $Y = \beta_0 + \beta_1 x + \epsilon$ to the data, as well as a separate cubic regression, i.e. $Y = \beta_0 + \beta_1X+\beta_2X^2 + \beta_3X^3 + \epsilon$.

(a). Suppose that the **true** relationship between X and Y is linear, i.e. $Y = \beta_0 + \beta_1X+\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, defined by 
$$RSS_{\text{train}} = \sum_{i=1}^n (y_i^{\text{train}}-\hat{y}_i)^2 $$
and also the training RSS for the cubic regression. Would we expect one to be lower than the other,  would we expect them to be the same, or is there not enough information to tell? Justify your answer.

(b). Answer (a) using **test** rather than training RSS.

(c).  Suppose that the **true** relationship between X and Y is **not** linear, but we don't know how far it is from linear. Consider the training RSS  for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other,  would we expect them to be the same, or is there not enough information to tell? Justify your answer.

(d). Answer (c) using test rather than training RSS.


## Part 2

(a). Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $i$th fitted value takes the form $$\hat{y}_i = x_i \hat{\beta},$$
where

$$\hat\beta= \frac{ \sum_{i=1}^n x_i y_i }{ \sum_{i=1}^n x_{i}^2 }$$

Show that we can write $$\hat{y}_i = \sum_{j=1}^n a_{i,j} y_{j}.$$
What is $a_{i,j}$? 

<!-- (b). Repeat part (a), now including an intercept.  -->
(b). Now, consider the case where we include an intercept, so the model is $y = \beta_0 + \beta_1 x + \epsilon$. For this question, you can use the formulae
$$
\begin{aligned}
\hat{\beta}_1 &= \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
{\sum_{i=1}^n (x_i-\bar{x})^2} \\
\hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \cdot \bar{x}.
\end{aligned}
$$
<!-- Conclude that, in both cases, the fitted values are found -->
<!-- by multiplying $y$ by a matrix $A$ where the matrix $A$ depends on $X$. -->
<!-- That is, -->
<!-- $$ -->
<!-- \hat{y} = A y. -->
<!-- $$ -->
<!-- (c).  -->
Show that in the case of a linear regression model with intercept, the regression line passes through the point $(\bar x, \bar y)$. Is this also true
for the regression without an intercept?


## Part 3

This question involves the use of multiple linear regression on the Auto data set.
You can get this from the ISLR package in R (install from CRAN)
```{r message=FALSE, warning=FALSE}
pacman::p_load(ISLR)
data(Auto)
```

(a). Produce a bivariate plot matrix using e.g. `GGally` or base R graphics which includes all of the variables in the data set except `name`. 
```{r warning=FALSE, message=FALSE}
pacman::p_load(dplyr)
if (!require(GGally)) {install.packages(GGally,type='source')}
```

(b).   Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.

(c).  Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:

```{r include=F}
lm.fit <- lm(mpg~.-name,data=Auto)
summary(lm.fit)
```


- Is there a relationship between the predictors and the response?
- Which predictors appear to have a statistically significant relationship to the response?
- What does the coefficient for the `year` variable suggest?
- Interpret the coefficient on weight. What does the sign mean? What does the magnitude mean? (in this case you should be able to interpret it exactly -- look up the units for the different variables here https://cran.r-project.org/web/packages/ISLR/ISLR.pdf) 

(d). Use the `plot` function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?

```{r include = FALSE}
par(mfrow=c(2,2))
plot(lm.fit)
```

(e). Try a few different transformations of the variables, such
as $\log(X)$, $\sqrt{X}$, $X^2$. Comment on your findings.


