---
title:  "Sampling Variability Lab 2"
author: "Data Science Team"
output: 
     html_document:
         keep_md: true
---

```{r setup}
source('https://stats101.stanford.edu/profile.R')
knitr::opts_chunk$set(dev="png",dpi=300, out.width=500,  fig.align="center",fig.width=5,fig.height=4)
```

### Sampling variability as a function of sample size

Let's work again with the uniform distribution U[50,100] as a model for our unobserved population and let's imagine that we want to estimate its mean (which we know is 75). Just as a refresher, let's plot the density and a histogram of 10000 observations from U[50,100].

```{r,fig.width=9,fig.height=4,out.width=700}
par(mfrow=c(1,2))
x<-seq(40,110,length.out = 1000)
plot(x,dunif(x,min=50,max=100),main="Density of a uniform U[50,100]",ylab="Density",pty="l")
X<-runif(10000,min=50,max=100)
hist(X,main="Histogram of 1000 samples from U[50,100]",xlim=c(40,110))
abline(v=mean(X),lwd=3,col="red")
```

Now, let's get down to business and see how the sample size affects the variability of our estimate.
First we look at just three cases and plot histograms.
```{r,fig.width=7,fig.height=2.5,out.width=900}
ssize<-c(10,50,100)
B<-1000
smean<-matrix(NA,nrow=B,ncol=length(ssize))
for(i in 1:B)
{
  for(j in 1:length(ssize))
      {
      X<-runif(ssize[j],min=50,max=100)
      smean[i,j]<-mean(X)
  }
}
par(mfrow=c(1,3))
for(i in 1:3){
hist(smean[,i],freq = FALSE,main="Histogram of 1000 sample means",xlab=paste("Mean of a sample of size",as.character(ssize[i])),xlim=c(60,90))}
```

Now let's actually consider a few more cases and quantify varibility with standard deviation. While we are at it, we also calculate the mean value of the estimates for each of the sample sizes.

We are going to consider 10 different sample sizes
```{r}
ssize<-c(10,25,50,75,100,150,500,1000,5000,10000)
B<-1000
smean<-matrix(NA,nrow=B,ncol=length(ssize))
for(i in 1:B)
{
  for(j in 1:length(ssize))
      {
      X<-runif(ssize[j],min=50,max=100)
      smean[i,j]<-mean(X)
  }
}
mean.sample<-apply(smean,2,mean)
sd.sample<-sqrt(apply(smean,2,var))
```

Now let's look at the average and standard deviation of our estimates as a function of sample size
```{r,fig.width=8,fig.height=4,out.width=700}
par(mfrow=c(1,2))
plot(ssize,mean.sample,xlab="Sample Size",ylab="",main="Average value of estimate")
plot(ssize,sd.sample,xlab="Sample Size",ylab="",main="Standard deviation of estimate")
```

Because sample sizes are not equally spread we do not have the best display. It might be useful to change scale for the x-axis.

```{r,fig.width=8,fig.height=4,out.width=700}
par(mfrow=c(1,2))
plot(log(ssize),mean.sample,xlab="log(Sample Size)",ylab="",main="Average value of estimate")
plot(log(ssize),sd.sample,xlab="log(Sample Size)",ylab="",main="Standard deviation of estimate")
```

```{r,fig.width=8,fig.height=4,out.width=700}
par(mfrow=c(1,2))
plot(sqrt(ssize),mean.sample,xlab="sqrt(Sample Size)",ylab="",main="Average value of estimate")
plot(sqrt(ssize),sd.sample,xlab="sqrt(Sample Size)",ylab="",main="Standard deviation of estimate")
```

### Sampling variability as a function of the population variance

We want to do a similar exercise, but this time we want to explore how the variability of our estimate of the mean depends on the population variance.
Let's think about how we could vary the parameters of a Uniform distribution to change the variance but keep the mean constant: we want to change the spread around the central value.

Again we start by looking at three cases with histograms

```{r,fig.width=7,fig.height=2.5,out.width=900}
Umin<-c(10,40,60)
B<-1000
smean<-matrix(NA,nrow=B,ncol=length(Umin))
for(i in 1:B)
{
  for(j in 1:length(Umin))
      {
      X<-runif(50,min=Umin[j],max=75+75-Umin[j])
      smean[i,j]<-mean(X)
  }
}
par(mfrow=c(1,3))
for(i in 1:3)
{hist(smean[,i],freq = FALSE,xlab="Sample mean",main=paste("Samples from Unif",paste(as.character(Umin[i]), as.character(150-Umin[i]),sep="-")),xlim=c(60,90))}
```

Now let's look at a few more values for the range of the uniform distribution

```{r,fig.width=7,fig.height=2.5,out.width=900}
Umin<-c(1,10,20,30,40,50,60,70)
B<-1000
smean<-matrix(NA,nrow=B,ncol=length(Umin))
for(i in 1:B)
{
  for(j in 1:length(Umin))
      {
      X<-runif(50,min=Umin[j],max=75+75-Umin[j])
      smean[i,j]<-mean(X)
  }
}
mean.sample<-apply(smean,2,mean)
sd.sample<-sqrt(apply(smean,2,var))
```
```{r,fig.width=8,fig.height=4,out.width=700}
par(mfrow=c(1,2))
plot(2*(75-Umin),mean.sample,xlab="Range of possible values",ylab="",main="Average value of estimate")
plot(2*(75-Umin),sd.sample,xlab="Range of possible values",ylab="",main="Standard deviation of estimate")
```

### Practice with the Bootstrap

We are going to work with a dataset that explores the connection between propensity to use the left hand and a genetic marker.
It is part of a library called `boot`, which contains an array of functions dealing with bootstrap. You should look at the information available with `help(boot)` once you have sourced the package.

```{r}
library(boot)
data("claridge")
summary(claridge)
```

#### 1. Look at the data

First let's plot the data so that we have a basic understanding of what is going on.

```{r,fig.width=5,fig.height=4,out.width=450}
library(ggplot2)
ggplot(claridge,aes(x=dnan,y=hand))+geom_count(alpha=0.5,col="blue")+ylab("Propensity to use left hand")+xlab("DNA variant")+ggtitle("Claridge data")
```

#### 2. Does the DNA measure have anything to do with hand use?

Say that we are interested in evaluating if there is a relation between propensity to use the left hand and the DNA variant in question.
One summary statistics that we might be interested in is the **correlation** between the counts of this DNA variant and the propensity to use the left hand. We have not formally introduced this summary in class yet, but you might recall the notion of correlation from previous studies. The correlation is an **index** of dependence between to variables (X and Y), which has absolute value 1 when one of the two variables can be expressed exactly as a linear function (Y=a +bX) of the other. The value is negative or positive depending on the sign of coefficient b. The correlation is 0 when there is no evidence for any linear relation between Y and X.

The command you want to use in R is `cor` and the first thing to do is to look up the help with `help(cor)`

To have a sense of what the correlation does, consider the following  pairs of variables.

- X ~ N(0,10), Y1=5+3*X
- X ~ N(0,10), Z~ N(0,5), Y2=5+3*X + Z
- X ~ N(0,10), E~ N(0,20), Y3=5+3*X + E
- X ~ N(0,10), Y4~ N(5,30)

First we want to generate a sample of size 1000 for each, plot the values we generate and calculate their correlation. Recall that to generate samples from a normal distribution we use the function `rnorm`.


```{r,fig.width=9,fig.height=2.3,out.width=900}

X<-rnorm(1000,mean=0,sd=10)
Y1<-X*3+5
Y2<-X*3+5+rnorm(1000,mean=0,sd=5)
Y3<-X*3+5+rnorm(1000,mean=0,sd=20)
Y4<-rnorm(1000,mean=5,sd=30)
par(mfrow=c(1,4))
plot(X,Y1,main=paste("correlation=",as.character(round(cor(X,Y1),digits=3))))
plot(X,Y2,main=paste("correlation=",as.character(round(cor(X,Y2),digits=3))))
plot(X,Y3,main=paste("correlation=",as.character(round(cor(X,Y3),digits=3))))
plot(X,Y4,main=paste("correlation=",as.character(round(cor(X,Y4),digits=3))))
```

Then, let's re-do  the same experiment but with a sample of size 37, which is the size of our dataset.

```{r,fig.width=9,fig.height=2.3,out.width=900}

X<-rnorm(37,mean=0,sd=10)
Y1<-X*3+5
Y2<-X*3+5+rnorm(37,mean=0,sd=5)
Y3<-X*3+5+rnorm(37,mean=0,sd=20)
Y4<-rnorm(37,mean=5,sd=30)
par(mfrow=c(1,4))
plot(X,Y1,main=paste("correlation=",as.character(round(cor(X,Y1),digits=3))))
plot(X,Y2,main=paste("correlation=",as.character(round(cor(X,Y2),digits=3))))
plot(X,Y3,main=paste("correlation=",as.character(round(cor(X,Y3),digits=3))))
plot(X,Y4,main=paste("correlation=",as.character(round(cor(X,Y4),digits=3))))
```


```{r}
```

Let's now evaluate the correlation in our dataset.

```{r}
sample.cor<-cor(claridge$dnan,claridge$hand)
sample.cor
```

#### 3. How correlated are the variables?

Now we want to have a sense of how seriously we should take this value of the correlation and we resort to the bootstrap. Note that our sample consists in 37 pairs of observations. When we resample, we want to make sure that we keep the pairs together: they are the one that contain information about the correlation between the two variables. 

First a solution using only native R commands
```{r}
B<-10000
ssize<-nrow(claridge)
bcorr<-NULL
for(i in 1:B)
{
 bsample<-claridge[sample(1:ssize,ssize,replace=TRUE),] 
bcorr<-c(bcorr,cor(bsample[,1],bsample[,2]))
}
```

```{r}
hist(bcorr,main="Bootstrap variability",xlab="Correlation in bootstrap sample")
```

And now one using `dplyr`, which has a function `sample_n` which allow you to select rows from a table.

```{r}
library(dplyr)
correlations = vector()
for (i in 1:10000) {
  samples = sample_n(claridge, 37, replace = T)
  correlations[i] = cor(samples[,1], samples[,2])
}
correlations = data.frame(correlations)
```

```{r}
plot5 = ggplot(data = correlations, aes(x = correlations)) + geom_histogram(bins = 20,fill="cornflowerblue",alpha=.8) + labs(x = "Correlation in Bootstrap sample", title="Bootstrap variability")+ theme_bw()
plot5
```
